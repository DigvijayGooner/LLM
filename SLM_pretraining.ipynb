{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0cc81f4",
   "metadata": {},
   "source": [
    "### pretraining script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be7bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\SuperTails\\Supertails_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the BookCorpus dataset...\n",
      "Total rows in dataset: 74004228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading batches: 100%|██████████| 75/75 [08:32<00:00,  6.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 74004228\n",
      "    })\n",
      "})\n",
      "\n",
      "First example:\n",
      "{'text': 'the half-ling book one in the fall of igneeria series kaylee soderburg copyright 2013 kaylee soderburg all rights reserved .'}\n",
      "\n",
      "Complete dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset, DatasetDict\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# print(\"Loading the BookCorpus dataset...\")\n",
    "\n",
    "# # Load dataset in batches with progress bar\n",
    "# batch_size = 1000000  # Load 1 million rows at a time\n",
    "# dataset = load_dataset(\"SamuelYang/bookcorpus\")\n",
    "# total_rows = len(dataset['train'])\n",
    "\n",
    "# print(f\"Total rows in dataset: {total_rows}\")\n",
    "\n",
    "# # Initialize empty lists to store batches\n",
    "# all_data = []\n",
    "\n",
    "# # Process dataset in batches with progress bar\n",
    "# for start_idx in tqdm(range(0, total_rows, batch_size), desc=\"Loading batches\"):\n",
    "#     end_idx = min(start_idx + batch_size, total_rows)\n",
    "#     batch = dataset['train'].select(range(start_idx, end_idx))\n",
    "#     all_data.extend(batch['text'])\n",
    "\n",
    "# # Create final dataset\n",
    "# dataset = DatasetDict({\n",
    "#     'train': dataset['train'].select(range(total_rows))\n",
    "# })\n",
    "\n",
    "# print(\"\\nDataset structure:\")\n",
    "# print(dataset)\n",
    "\n",
    "# print(\"\\nFirst example:\")\n",
    "# print(dataset['train'][0])\n",
    "\n",
    "# print(\"\\nComplete dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd240742",
   "metadata": {},
   "source": [
    "loading the first 1cr rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\SuperTails\\Supertails_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the BookCorpus dataset...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"Loading the BookCorpus dataset...\")\n",
    "\n",
    "# Load dataset in batches with progress bar\n",
    "batch_size = 100000  # Load 1 million rows at a time\n",
    "dataset = load_dataset(\"SamuelYang/bookcorpus\")\n",
    "total_rows = min(1000000, len(dataset['train']))  # Limit to 1cr rows\n",
    "\n",
    "print(f\"Total rows in dataset: {total_rows}\")\n",
    "\n",
    "# Initialize empty lists to store batches\n",
    "all_data = []\n",
    "\n",
    "# Process dataset in batches with progress bar\n",
    "for start_idx in tqdm(range(0, total_rows, batch_size), desc=\"Loading batches\"):\n",
    "    end_idx = min(start_idx + batch_size, total_rows)\n",
    "    batch = dataset['train'].select(range(start_idx, end_idx))\n",
    "    all_data.extend(batch['text'])\n",
    "\n",
    "# Create final dataset with only 1cr rows\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['train'].select(range(total_rows))\n",
    "})\n",
    "\n",
    "print(\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "\n",
    "print(\"\\nFirst example:\")\n",
    "print(dataset['train'][0])\n",
    "\n",
    "print(\"\\nComplete dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6923a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets numpy tqdm ml_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887968bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\SuperTails\\Supertails_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "h:\\SuperTails\\Supertails_env\\lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set a new cache directory on your H: drive\n",
    "new_cache_dir = \"H:\\SuperTails\\huggingface_cache\" \n",
    "os.makedirs(new_cache_dir, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "# Set the environment variables\n",
    "os.environ['HF_HOME'] = new_cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = os.path.join(new_cache_dir, 'datasets')\n",
    "os.environ['TRANSFORMERS_CACHE'] = os.path.join(new_cache_dir, 'transformers')\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "class SimpleConfig:\n",
    "    # I/O\n",
    "    out_dir = 'out'\n",
    "    eval_interval = 200\n",
    "    log_interval = 1\n",
    "    eval_iters = 100\n",
    "    always_save_checkpoint = True\n",
    "\n",
    "    # Data\n",
    "    dataset = 'SamuelYang/bookcorpus'\n",
    "    gradient_accumulation_steps = 4\n",
    "    batch_size = 12\n",
    "    block_size = 256 # context of up to 256 previous characters\n",
    "\n",
    "    # Model\n",
    "    n_layer = 6\n",
    "    n_head = 6\n",
    "    n_embd = 384\n",
    "    dropout = 0.2\n",
    "    bias = False\n",
    "\n",
    "    # AdamW optimizer\n",
    "    learning_rate = 1e-3\n",
    "    max_iters = 2000\n",
    "    weight_decay = 1e-1\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.95\n",
    "    grad_clip = 1.0\n",
    "\n",
    "    # Learning rate decay settings\n",
    "    decay_lr = True\n",
    "    warmup_iters = 100\n",
    "    lr_decay_iters = 2000\n",
    "    min_lr = 1e-4\n",
    "\n",
    "# --- 2. Model Definition ---\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class Pet_SLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# --- 3. Data Loading and Preprocessing ---\n",
    "\n",
    "def process_and_save_data(config):\n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(config.dataset, split='train')\n",
    "\n",
    "    # Split dataset\n",
    "    train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    ds = DatasetDict({\n",
    "        'train': train_test_split['train'],\n",
    "        'validation': train_test_split['test']\n",
    "    })\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.add_special_tokens({'pad_token': '<PAD>', 'eos_token': '<EOS>'})\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, max_length=config.block_size, padding=\"max_length\")\n",
    "\n",
    "    tokenized_ds = ds.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=[\"text\"],\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "\n",
    "    for split, d in tokenized_ds.items():\n",
    "        arr_len = len(d)\n",
    "        filename = os.path.join(config.out_dir, f'{split}.bin')\n",
    "        dtype = np.uint16\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len, config.block_size))\n",
    "\n",
    "        for i, example in enumerate(tqdm(d, desc=f\"Writing {filename}\")):\n",
    "            arr[i] = example['input_ids']\n",
    "        arr.flush()\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "\n",
    "def get_lr(it, config):\n",
    "    if it < config.warmup_iters:\n",
    "        return config.learning_rate * it / config.warmup_iters\n",
    "    if it > config.lr_decay_iters:\n",
    "        return config.min_lr\n",
    "    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "def get_batch_from_stream(data_stream, tokenizer, batch_size, block_size, device):\n",
    "    \"\"\"\n",
    "    Generator function to yield batches of tokenized data from a streaming dataset.\n",
    "    \"\"\"\n",
    "    batch_texts = []\n",
    "    for example in data_stream:\n",
    "        # Append the text of the current example\n",
    "        batch_texts.append(example['text'])\n",
    "        \n",
    "        # If we have a full batch, process and yield it\n",
    "        if len(batch_texts) == batch_size:\n",
    "            # Tokenize the batch of texts\n",
    "            tokenized = tokenizer(\n",
    "                batch_texts, \n",
    "                truncation=True, \n",
    "                max_length=block_size, \n",
    "                padding=\"max_length\", \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            x = tokenized.input_ids\n",
    "            # Create the target sequence by shifting the input\n",
    "            y = torch.roll(x, shifts=-1, dims=1)\n",
    "            y[:, -1] = -1 # Ignore the last token in the target for loss calculation\n",
    "\n",
    "            # Move tensors to the correct device\n",
    "            if device == 'cuda':\n",
    "                x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "            else:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "            yield x, y\n",
    "            \n",
    "            # Clear the list for the next batch\n",
    "            batch_texts = []\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, config, device, val_data_loader, tokenizer): # Modified signature\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['validation']: # Only evaluate validation split for speed\n",
    "        losses = torch.zeros(config.eval_iters)\n",
    "        for k in range(config.eval_iters):\n",
    "            X, Y = next(val_data_loader) # Use the passed loader\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    # You can approximate train loss from the last training batch if needed\n",
    "    # to avoid creating a separate train data loader for estimation.\n",
    "    out['train'] = loss.item() * config.gradient_accumulation_steps \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train_model(config, train_data_loader, val_data_loader, tokenizer): # <-- Accept data loaders and tokenizer\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"--- Starting training on device: {device.upper()} ---\") \n",
    "    \n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=torch.bfloat16)\n",
    "\n",
    "    # Use the tokenizer's vocab size, which includes special tokens\n",
    "    model_config = ModelConfig(\n",
    "        n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd,\n",
    "        block_size=config.block_size, bias=config.bias, vocab_size=len(tokenizer), dropout=config.dropout\n",
    "    )\n",
    "    model = Pet_SLM(model_config)\n",
    "    model.to(device)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(torch.float16 == torch.bfloat16))\n",
    "    optimizer = model.configure_optimizers(config.weight_decay, config.learning_rate, (config.beta1, config.beta2), device)\n",
    "    \n",
    "    iter_num = 0\n",
    "    best_val_loss = 1e9\n",
    "\n",
    "    for iter_num in tqdm(range(config.max_iters), desc=\"Training\"):\n",
    "        lr = get_lr(iter_num, config) if config.decay_lr else config.learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        if iter_num > 0 and iter_num % config.eval_interval == 0:\n",
    "            losses = estimate_loss(model, config, device, val_data_loader, tokenizer) # Pass loader and tokenizer\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['validation']:.4f}\")\n",
    "            if losses['validation'] < best_val_loss:\n",
    "                best_val_loss = losses['validation']\n",
    "        \n",
    "        for _ in range(config.gradient_accumulation_steps):\n",
    "            # Get the next batch from the data loader generator\n",
    "            X, Y = next(train_data_loader)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / config.gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "        \n",
    "        if config.grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # --- SAVE FINAL MODEL AT THE END ---\n",
    "    final_checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'model_args': model_config,\n",
    "        'config': config,\n",
    "    }\n",
    "    print(f\"\\nTraining complete. Saving final model to {config.out_dir}\")\n",
    "    torch.save(final_checkpoint, os.path.join(config.out_dir, 'final_model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d8d78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data streams...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\SuperTails\\Supertails_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in H:\\SuperTails\\huggingface_cache\\transformers\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting training on device: CUDA ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6884\\928683569.py:369: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(torch.float16 == torch.bfloat16))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 29.92M\n",
      "num decayed parameter tensors: 26, with 30,014,592 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 126/2000 [00:11<02:54, 10.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m\n\u001b[0;32m     31\u001b[0m val_data_loader \u001b[38;5;241m=\u001b[39m get_batch_from_stream(\n\u001b[0;32m     32\u001b[0m     cycle(val_stream), tokenizer, config\u001b[38;5;241m.\u001b[39mbatch_size, config\u001b[38;5;241m.\u001b[39mblock_size, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# --- 3. START TRAINING ---\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcess finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 388\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config, train_data_loader, val_data_loader, tokenizer)\u001b[0m\n\u001b[0;32m    384\u001b[0m         best_val_loss \u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps):\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;66;03m# Get the next batch from the data loader generator\u001b[39;00m\n\u001b[1;32m--> 388\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m    390\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n",
      "Cell \u001b[1;32mIn[2], line 311\u001b[0m, in \u001b[0;36mget_batch_from_stream\u001b[1;34m(data_stream, tokenizer, batch_size, block_size, device)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# If we have a full batch, process and yield it\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_texts) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;66;03m# Tokenize the batch of texts\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     x \u001b[38;5;241m=\u001b[39m tokenized\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# Create the target sequence by shifting the input\u001b[39;00m\n",
      "File \u001b[1;32mh:\\SuperTails\\Supertails_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2911\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2911\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2913\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mh:\\SuperTails\\Supertails_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2999\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   2994\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2995\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2996\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2997\u001b[0m         )\n\u001b[0;32m   2998\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   3000\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3001\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3002\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3003\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   3004\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3005\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3006\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3007\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3008\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   3009\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3010\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3011\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3012\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3013\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3014\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3015\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3016\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3017\u001b[0m         split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   3018\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3019\u001b[0m     )\n\u001b[0;32m   3020\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   3022\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3023\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3041\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3042\u001b[0m     )\n",
      "File \u001b[1;32mh:\\SuperTails\\Supertails_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3200\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3190\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3191\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3192\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3193\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3197\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3198\u001b[0m )\n\u001b[1;32m-> 3200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3201\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3202\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3203\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3204\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3205\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3206\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3207\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3208\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3209\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   3210\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3211\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3212\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3213\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3214\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3215\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3216\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3217\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3218\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   3219\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3220\u001b[0m )\n",
      "File \u001b[1;32mh:\\SuperTails\\Supertails_env\\lib\\site-packages\\transformers\\tokenization_utils.py:889\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 889\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    891\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32mh:\\SuperTails\\Supertails_env\\lib\\site-packages\\transformers\\tokenization_utils.py:854\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 854\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mh:\\SuperTails\\Supertails_env\\lib\\site-packages\\transformers\\tokenization_utils.py:697\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 697\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    698\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32mh:\\SuperTails\\Supertails_env\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py:277\u001b[0m, in \u001b[0;36mGPT2Tokenizer._tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Tokenize a string.\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m bpe_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    278\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     )  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     bpe_tokens\u001b[38;5;241m.\u001b[39mextend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbpe(token)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mh:\\SuperTails\\Supertails_env\\lib\\site-packages\\regex\\regex.py:338\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags, pos, endpos, overlapped, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all matches in the string. The matches may be overlapped\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mif overlapped is True. If one or more groups are present in the pattern,\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03mreturn a list of groups; this will be a list of tuples if the pattern has\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mmore than one group. Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m pat \u001b[38;5;241m=\u001b[39m _compile(pattern, flags, ignore_unused, kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = SimpleConfig()\n",
    "    os.makedirs(config.out_dir, exist_ok=True)\n",
    "    \n",
    "    # --- 1. SETUP DATA STREAMS (No saving to disk) ---\n",
    "    print(\"Setting up data streams...\")\n",
    "    # It's safer to use forward slashes for paths\n",
    "    new_cache_dir = \"H:/SuperTails/huggingface_cache\" \n",
    "    os.makedirs(new_cache_dir, exist_ok=True)\n",
    "    os.environ['HF_HOME'] = new_cache_dir\n",
    "    os.environ['HF_DATASETS_CACHE'] = os.path.join(new_cache_dir, 'datasets')\n",
    "\n",
    "    # Load dataset in streaming mode\n",
    "    full_dataset = load_dataset(config.dataset, split='train', streaming=True)\n",
    "    \n",
    "    # Split the stream for train and validation\n",
    "    train_stream = full_dataset.take(70000000) # Use a large portion for training\n",
    "    val_stream = full_dataset.skip(70000000).take(10000) # Use a small portion for validation\n",
    "\n",
    "    # Initialize tokenizer and add special tokens\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.add_special_tokens({'pad_token': '<PAD>', 'eos_token': '<EOS>'})\n",
    "    # Set the pad_token_id for the tokenizer\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('<PAD>')\n",
    "\n",
    "    # --- 2. CREATE DATA LOADERS ---\n",
    "    # Use itertools.cycle to endlessly loop over the stream\n",
    "    train_data_loader = get_batch_from_stream(\n",
    "        cycle(train_stream), tokenizer, config.batch_size, config.block_size, 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    val_data_loader = get_batch_from_stream(\n",
    "        cycle(val_stream), tokenizer, config.batch_size, config.block_size, 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    # --- 3. START TRAINING ---\n",
    "    train_model(config, train_data_loader, val_data_loader, tokenizer)\n",
    "\n",
    "    print(\"\\nProcess finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf114e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee2050f7",
   "metadata": {},
   "source": [
    "checking if its running on GPU or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab94100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA-enabled GPU(s).\n",
      "✅ --- Successfully using GPU 0: NVIDIA GeForce RTX 4090 --- ✅\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Found {device_count} CUDA-enabled GPU(s).\")\n",
    "\n",
    "    # Get the name of the current GPU\n",
    "    current_device_index = torch.cuda.current_device()\n",
    "    gpu_name = torch.cuda.get_device_name(current_device_index)\n",
    "    \n",
    "    print(f\"✅ --- Successfully using GPU {current_device_index}: {gpu_name} --- ✅\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ --- PyTorch cannot find a CUDA-enabled GPU. Running on CPU. --- ❌\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d7689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n",
      "Number of GPUs available: 1\n",
      "Current GPU index: 0\n",
      "Current GPU name: NVIDIA GeForce RTX 4090\n",
      "PyTorch version: 2.7.1+cu118\n",
      "PyTorch built with CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Is CUDA available?\n",
    "is_available = torch.cuda.is_available()\n",
    "print(f\"Is CUDA available? {is_available}\")\n",
    "\n",
    "if is_available:\n",
    "    # 2. How many GPUs can PyTorch see?\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {device_count}\")\n",
    "\n",
    "    # 3. What is the name of the current GPU?\n",
    "    current_device = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current GPU index: {current_device}\")\n",
    "    print(f\"Current GPU name: {device_name}\")\n",
    "else:\n",
    "    print(\"PyTorch cannot find a CUDA-enabled GPU.\")\n",
    "\n",
    "# 4. What version of PyTorch and CUDA is it built with?\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if is_available:\n",
    "    print(f\"PyTorch built with CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n",
      "Great! CUDA is available.\n",
      "PyTorch version: 2.7.1+cu118\n",
      "PyTorch built with CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Is CUDA available?\n",
    "is_available = torch.cuda.is_available()\n",
    "print(f\"Is CUDA available? {is_available}\")\n",
    "\n",
    "if not is_available:\n",
    "    print(\"PyTorch cannot find a CUDA-enabled GPU. This is the root of the problem.\")\n",
    "else:\n",
    "    print(f\"Great! CUDA is available.\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"PyTorch built with CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf3187b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494486b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca5a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c6f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c2863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "223bd04f",
   "metadata": {},
   "source": [
    "###  Fine-Tuning Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0422486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "import json # New: for loading jsonl file\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# --- Configuration (Adjusted for Fine-tuning) ---\n",
    "\n",
    "class SimpleConfig:\n",
    "    # I/O\n",
    "    out_dir = 'out'\n",
    "    eval_interval = 50   # Evaluate more often\n",
    "    log_interval = 1\n",
    "    eval_iters = 20\n",
    "    always_save_checkpoint = True\n",
    "    \n",
    "    # New: Path to the pre-trained model\n",
    "    init_from = 'resume' # Can be 'scratch' or 'resume'\n",
    "    pretrained_ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "\n",
    "    # Data\n",
    "    finetune_data_path = 'unified.jsonl' # New: Path to your jsonl file\n",
    "    gradient_accumulation_steps = 1\n",
    "    batch_size = 4  # Use a smaller batch size\n",
    "    block_size = 256\n",
    "\n",
    "    # Model (keep the same as pre-training)\n",
    "    n_layer = 6\n",
    "    n_head = 6\n",
    "    n_embd = 384\n",
    "    dropout = 0.2\n",
    "    bias = False\n",
    "\n",
    "    # AdamW optimizer (Adjusted for Fine-tuning)\n",
    "    learning_rate = 3e-5  # Much lower learning rate\n",
    "    max_iters = 200      # Fewer iterations are needed\n",
    "    weight_decay = 1e-1\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.95\n",
    "    grad_clip = 1.0\n",
    "\n",
    "    # Learning rate decay settings\n",
    "    decay_lr = True\n",
    "    warmup_iters = 20\n",
    "    lr_decay_iters = 200 # Should be same as max_iters\n",
    "    min_lr = 3e-6        # Lower min learning rate\n",
    "\n",
    "# --- Model Definition (Same as before) ---\n",
    "# ... (The Pet_SLM model and its components are identical, so they are omitted for brevity)\n",
    "# ... (Just copy the entire Model Definition section from the previous script here)\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = SelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class Pet_SLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "# --- NEW: Data Loading and Preprocessing for Fine-tuning ---\n",
    "\n",
    "def prepare_finetuning_data(config):\n",
    "    # Load and process the .jsonl file\n",
    "    all_text = []\n",
    "    with open(config.finetune_data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # The <EOS> token is a separator, not part of the JSON\n",
    "            json_str = line.strip().replace('<EOS>', '').strip()\n",
    "            if json_str:\n",
    "                data = json.loads(json_str)\n",
    "                all_text.append(data.get('text', '')) # Safely get the text\n",
    "\n",
    "    # Split data (90% train, 10% validation)\n",
    "    train_size = int(0.9 * len(all_text))\n",
    "    train_data = all_text[:train_size]\n",
    "    val_data = all_text[train_size:]\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token # set pad token\n",
    "\n",
    "    # Tokenize and save to binary files\n",
    "    for split, data in [('train', train_data), ('validation', val_data)]:\n",
    "        tokenized_texts = tokenizer(data, truncation=True, max_length=config.block_size, padding=\"max_length\", return_tensors=\"np\")\n",
    "        \n",
    "        filename = os.path.join(config.out_dir, f'{split}_finetune.bin')\n",
    "        dtype = np.uint16\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=tokenized_texts['input_ids'].shape)\n",
    "        \n",
    "        arr[:] = tokenized_texts['input_ids']\n",
    "        arr.flush()\n",
    "        print(f\"Saved {split} data to {filename}\")\n",
    "\n",
    "\n",
    "# --- Training Loop (Adapted for Fine-tuning) ---\n",
    "\n",
    "def get_lr(it, config):\n",
    "    if it < config.warmup_iters:\n",
    "        return config.learning_rate * it / config.warmup_iters\n",
    "    if it > config.lr_decay_iters:\n",
    "        return config.min_lr\n",
    "    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n",
    "\n",
    "def get_batch(split, config, device):\n",
    "    # Load from the new fine-tuning binary files\n",
    "    data_path = os.path.join(config.out_dir, f'{split}_finetune.bin')\n",
    "    data = np.memmap(data_path, dtype=np.uint16, mode='r')\n",
    "    \n",
    "    # Reshape data to (num_samples, block_size)\n",
    "    data = data.reshape(-1, config.block_size)\n",
    "\n",
    "    # Get random indices\n",
    "    ix = torch.randint(len(data), (config.batch_size,))\n",
    "    \n",
    "    # Grab the batches\n",
    "    x = torch.from_numpy(data[ix]).to(torch.int64)\n",
    "    y = torch.from_numpy(data[ix]).to(torch.int64) # In causal LM, y is the same as x\n",
    "    \n",
    "    # Move to device\n",
    "    if device == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, config, device):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'validation']:\n",
    "        losses = torch.zeros(config.eval_iters)\n",
    "        for k in range(config.eval_iters):\n",
    "            X, Y = get_batch(split, config, device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def finetune_model(config):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=torch.bfloat16)\n",
    "\n",
    "    # --- Model Loading ---\n",
    "    if config.init_from == 'resume':\n",
    "        print(f\"Resuming training from {config.pretrained_ckpt_path}\")\n",
    "        checkpoint = torch.load(config.pretrained_ckpt_path, map_location=device)\n",
    "        model_args = checkpoint['model_args']\n",
    "        model = Pet_SLM(model_args)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        print(\"Initializing a new model from scratch\")\n",
    "        model_config = ModelConfig(\n",
    "            n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd,\n",
    "            block_size=config.block_size, bias=config.bias, vocab_size=50257, dropout=config.dropout\n",
    "        )\n",
    "        model = Pet_SLM(model_config)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(torch.float16 == torch.bfloat16))\n",
    "    optimizer = model.configure_optimizers(config.weight_decay, config.learning_rate, (config.beta1, config.beta2), device)\n",
    "    \n",
    "    iter_num = 0\n",
    "    best_val_loss = 1e9\n",
    "    if config.init_from == 'resume': # If resuming, load optimizer state as well\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        iter_num = checkpoint['iter_num']\n",
    "\n",
    "\n",
    "    for iter_num in tqdm(range(config.max_iters), desc=\"Fine-tuning\"):\n",
    "        lr = get_lr(iter_num, config) if config.decay_lr else config.learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        if iter_num % config.eval_interval == 0 and iter_num > 0:\n",
    "            losses = estimate_loss(model, config, device)\n",
    "            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['validation']:.4f}\")\n",
    "            if losses['validation'] < best_val_loss or config.always_save_checkpoint:\n",
    "                best_val_loss = losses['validation']\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {config.out_dir}\")\n",
    "                # Save to a new fine-tuned checkpoint file\n",
    "                torch.save(checkpoint, os.path.join(config.out_dir, 'ckpt_finetuned.pt'))\n",
    "        \n",
    "        for _ in range(config.gradient_accumulation_steps):\n",
    "            X, Y = get_batch('train', config, device)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / config.gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "        \n",
    "        if config.grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == '__main__':\n",
    "#     config = SimpleConfig()\n",
    "#     os.makedirs(config.out_dir, exist_ok=True)\n",
    "    \n",
    "#     # 1. Process and save the new fine-tuning data\n",
    "#     prepare_finetuning_data(config)\n",
    "    \n",
    "#     # 2. Run the fine-tuning process\n",
    "#     finetune_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c0c534",
   "metadata": {},
   "source": [
    "continue running from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleConfig()\n",
    "os.makedirs(config.out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Process and save the new fine-tuning data\n",
    "prepare_finetuning_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46dd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run the fine-tuning process\n",
    "finetune_model(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Supertails_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
